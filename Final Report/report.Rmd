---
title: "STAT 557 Final Project Report"
author: "Sarah Shy, Jonathan Hehir"
date: "Spring 2020"
geometry: margin=1in
fontfamily: charter
fontsize: 12pt
bibliography: sources.bib
csl: american-statistical-association.csl
#classoption:
#  - twocolumn
numbersections: true
output:
  bookdown::pdf_document2:
    toc: true
    toc_depth: 1
    template: null
abstract: |
  This is a test. This is a test. This is a test. This is a test. This is a test.
  This is a test. This is a test. This is a test. This is a test. This is a test.
---


\setlength\abovedisplayskip{0pt}

```{r global_options, include = FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.width = 5,
                      fig.height = 3)
```

```{r}
# Front matter
library(cowplot)
library(tidyverse)
library(knitr)
```

<!--
Game Plan (to be deeleeted):

* Abstract
* Intro:
    + what is measurement error and why we should use it for uncertainty quantification
    + what is gaussian perturbation and what we get from it?
        - uncertainty quantification (important if we observe only one noisy instance, ideally measure many times)
        - stability (important for reproducibility, trustworthiness, more bs like that)
        - soft classification (for classifiers that don't have probabilities)
* Formal Gaussian perturbation pipeline
* Toy example for illustration
* Astro data 
* Discussion:
    + Recap what we have
* Limitations
* Future Directions and Related Work
* Bib

-->

# Introduction

Measurement error is the difference between a measured quantity and its true value [@dodge2006oxford]. Generally, several measurements on the same quantity on the same subject will not be the same. This variation may arise due to variation in the measurement process, the inherent variation in the subject, or both [@bland1996statistics]. In this report, we focus on the former, assuming that the true value of a quantity of interest is fixed. Furthermore, we rule out systematic error, which may arise with inconsistent calibration, and focus only on random error.

In many statistical scenarios, we assume that our data are perfect measurements of a quantity of interest. This often false assumption can lead to bias in parameter estimation for statistical models, loss of power for detecting interesting variable relationships, and masking of features of the data [@carroll2006measurement]. It is therefore crutial for us to investigate how measurement error influences modeling and to quantify uncertainty in statistical inference and estimation

In many contexts, we know the variation of our measurement error. This is particularly true in various scientific and engineering settings, where the random error of a measuring device can be measured by the manufacturer or the scientist. In other settings, when the measurement error is unknown, but one can take repeated measurements on each subject, we can estimate the within-subject variation [@bland1996statistics]. In this paper, we discuss how we can use the known or estimated variance of measurement error to study the stability of a classification algorithm and quantify uncertainty in classification results. This report summarizes our exploratory work in this area of research. While existing research on uncertainty quantification is largely algorithm specific, we're exploring a general pipeline for quantifying uncertainty with any classifier. This would allow us to better compare uncertainty and stability across multiple classifiers.

In Section \@ref(pipeline), we introduce the Gaussian perturbation pipeline for stability and uncertainty quantification. In Section \@ref(toyexample), we apply the pipeline on simulated toy data. In Section \@ref(astro), we present a real-world application to Astronomy data.

# Gaussian Perturbation Pipeline {#pipeline}

![Gaussian perturbation pipeline for uncertainty quantification](../Presentation/pipeline.png)

# Toy Example {#toyexample}

## Toy Example: Set up

We generated data representing true values of a sample from the following

$$
\begin{aligned}
\mathbf X_{1}, \dots, \mathbf X_{250} &\sim \text{i.i.d. } \mathcal N \left( \begin{bmatrix} 1 \\ 2 \end{bmatrix}, \begin{bmatrix} 3 & -0.5 \\ -0.5 & 2 \end{bmatrix} \right) \\
\mathbf X_{251}, \dots, \mathbf X_{500} &\sim \text{i.i.d. }\mathcal N \left( \begin{bmatrix} -1 \\ -1 \end{bmatrix}, \begin{bmatrix} 4 & -1 \\ -1 & 1 \end{bmatrix} \right) \\
\end{aligned}
$$

Additionally, let us assume that the measurement error follows

$$
\varepsilon_i \sim \text{i.i.d. } \mathcal N \left( \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 & 0 \\ 0 & 3 \end{bmatrix} \right)
$$

That is, the values we observe each time we take a measurement of $\mathbf X$ is $\mathbf U = \mathbf X + \varepsilon$.

The true values and one instance of observed values are shown in Figure \@ref(fig:trueData).

```{r echo=F, warning=F}
toy <- new.env()
source("../Toys/svm_measurement_error_toy.R", toy)
attach(toy)

data <- make_base_data()
#kable(data %>% select(x1, x2) %>% add_column(error1 = 1, error2 = 3) %>% head(4), digits = 3)
```

```{r trueData, fig.cap = "The True Data"}
plt1 <- plot_data_with_error(data)

noisy <- noisify_data(data)
plt2 <- plot_data_with_error(noisy)

plot_grid(plt1, plt2)
```

## Toy Example: Linear Classifier on True Data and on Noisy Data

Fitting a linear SVM (tuned using 10-fold CV) to the above yields the following accuracies:

```{r include=F}
d_acc <- 100*svm_metrics(data)$accuracy
n_acc <- 100*svm_metrics(noisy)$accuracy

kable(tribble(~Dataset, ~Accuracy,
              "True", d_acc,
              "Measured", n_acc),
      "latex", digits = 4)
```

As expected, the linear classifier can better separate the classes in the true data. Some blah blah on this? In reality, we only have access to the noisy data. Can we trust this accuracy estimate having only observed one set of noisy measurements?

## Toy Example: 500 measurements

Using the same original dataset, we simulate taking noisy measurements and fitting the SVM 500 times to obtain the following distribution of classifier accuracy:

```{r cache=T, echo=F}
# simulate measurement error on one clean observation
sim_results_me <- run_simulations(data, 500, 1)

# simulate measurement error on one noisy observation
sim_results_noisy <- run_simulations(noisy, 500, 1)

#ggplot(sim_results_me) + geom_histogram(aes(x = accuracy), binwidth = 0.01, color = #"#000000", fill= "#ffaa33", size = .25) +
#  xlab("Accuracy") +
#  ylab("Frequency") +
#  theme_bw(base_size = 8)

ggplot(sim_results_me) + geom_boxplot(aes(y = accuracy), fill = "#ffaa33", size = 0.25) +
  ylab("Accuracy") +
  coord_flip() +
  theme_bw(base_size = 8) +
  theme(axis.text.y = element_blank())

detach(toy)
```

That is, we might observe this distribution if we could take 500 sets of measurements.

## Toy Example: Estimating Variance Without the True Values

```{r echo=F}
attach(toy)

ggplot(bind_rows(
  sim_results_me %>% add_column(observed = "500 Sets of Measurements"),
  sim_results_noisy %>% add_column(observed = "500 Perturbations of 1 Set of Measurements")
)) +
  geom_boxplot(aes(x = observed, y = accuracy), fill = c("#999999", "#ffaa33")) +
  xlab(" ") +
  ylab("Accuracy") +
  coord_flip() +
  theme_bw(base_size = 8)
```

- The standard deviation when taking 500 sets of measurements (what we want to estimate) is `r sprintf("%0.5f", sd(sim_results_me$accuracy))`.
- The standard deviation when performing Gaussian perturbation on one set of measurements is `r sprintf("%0.5f", sd(sim_results_noisy$accuracy))`.

## Toy Example: A Decision Boundary for Each Perturbed Dataset

```{r echo=F}
plot_data_with_decision_boundaries(noisy, sim_results_noisy)
```

## Toy Example: Soft Classification

Averaging predictions over those 500 decision boundaries yields a softened classifier:

```{r}
classified <- soft_classify_set(sim_results_noisy, noisy)

ggplot(classified) +
  geom_point(aes(x = x1, y = x2, color = p)) +
  scale_color_gradient(low = "#cc0011", high = "#223399") +
  xlim(-10, 10) +
  ylim(-10, 10)

# KILL THE TOY
detach(toy)
```

# Astronomy Application {#astro}

Classification method have been important tools in Astronomy for decades, and remain essential in studying the night sky [4]. 

# Discussion

# Bibliography
<!-- pls don't delete yet. I'll get to it, but I need to store it somewhere for now -->

[4] Kurtz, 1988. Astronomical Object Classification