---
title: "STAT 557 Final Project Report"
author: "Sarah Shy, Jonathan Hehir"
date: "Spring 2020"
geometry: margin=1in
fontfamily: charter
fontsize: 12pt
bibliography: sources.bib
csl: american-statistical-association.csl
#classoption:
#  - twocolumn
numbersections: true
output:
  bookdown::pdf_document2:
    toc: true
    toc_depth: 1
    template: null
abstract: |
  We consider the problem of classification in the presence of known measurement error. While the uncertainty imposed on the classifier by measurement error could in theory be estimated by taking many repeated measurements of the observed objects, this is usually not feasible in practice. In this report, we explore the possibility of assessing uncertainty in classification as well as robustness and stability of classification results in the presence of known measurement error by performing Gaussian perturbation on a single set of measurements.
---


\setlength\abovedisplayskip{0pt}

```{r global_options, include = FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.width = 6,
                      fig.height = 3)
```

```{r}
# Front matter
library(cowplot)
library(tidyverse)
library(knitr)
library(data.table)
library(knitr)
library(kableExtra)
set.seed(3)
theme_set(theme_bw(base_size = 10))
```

<!--
Game Plan (to be deeleeted):

* Abstract
* Intro:
    + what is measurement error and why we should use it for uncertainty quantification
    + what is gaussian perturbation and what we get from it?
        - uncertainty quantification (important if we observe only one noisy instance, ideally measure many times)
        - stability (important for reproducibility, trustworthiness, more bs like that)
        - soft classification (for classifiers that don't have probabilities)
* Formal Gaussian perturbation pipeline
* Toy example for illustration
* Astro data 
* Discussion:
    + Recap what we have
* Limitations
* Future Directions and Related Work
* Bib

-->

# Introduction

Measurement error is the difference between a measured quantity and its true value [@dodge2006oxford]. Generally, several measurements on the same quantity on the same subject will not be the same. This variation may arise due to variation in the measurement process, the inherent variation in the subject, or both [@bland1996statistics]. In this report, we focus on the former, assuming that the true value of a quantity of interest is fixed. Furthermore, we rule out systematic error, which may arise with inconsistent calibration, and focus only on random error.

In many statistical scenarios, we assume that our data are perfect measurements of a quantity of interest. This often false assumption can lead to bias in parameter estimation for statistical models, loss of power for detecting interesting variable relationships, and masking of features of the data [@carroll2006measurement]. It is therefore crutial for us to investigate how measurement error influences modeling and to quantify uncertainty in statistical inference and estimation.

In many contexts, we know the variation of our measurement error. This is particularly true in various scientific and engineering settings, where the random error of a measuring device can be measured by the manufacturer or the scientist. In other settings, when the measurement error is unknown, but one can take repeated measurements on each subject, we can estimate the within-subject variation [@bland1996statistics]. In this paper, we focus on the use of known measurement error, although the results would apply in a similar fashion to estimated measurement error. 

The goal of this paper is to assess the stability of a classification algorithm and quantify uncertainty in classification results in the presence of measurement error. While existing research on uncertainty quantification is largely algorithm-specific, our work explores a general pipeline for quantifying uncertainty with any classifier. This would allow us to better compare uncertainty, stability, and overall performance across multiple classifiers. This report summarizes our exploratory work in this area of research.

We begin by motivating the subject with a toy example in Section \@ref(motivation). In Section \@ref(pipeline), we introduce the Gaussian perturbation pipeline for stability and uncertainty quantification. In Section \@ref(toyexample), we apply the pipeline to the previously simulated toy data. In Section \@ref(astro), we present a real-world application to Astronomy data. Finally, we discuss the results and future direction in Section \@ref(discussion).

# Motivation {#motivation}

In order to illustrate the relevant issues that arise in the presence of measurement error, we begin by introducing a toy example. We will use a set of data representing true values belonging to two classes by sampling 250 points each from two bivariate normal distributions:

$$
\begin{aligned}
\mathbf X_{1}, \dots, \mathbf X_{250} &\sim \text{i.i.d. } \mathcal N \left( \begin{bmatrix} 1 \\ 2 \end{bmatrix}, \begin{bmatrix} 3 & -0.5 \\ -0.5 & 2 \end{bmatrix} \right) \\
\mathbf X_{251}, \dots, \mathbf X_{500} &\sim \text{i.i.d. }\mathcal N \left( \begin{bmatrix} -1 \\ -1 \end{bmatrix}, \begin{bmatrix} 4 & -1 \\ -1 & 1 \end{bmatrix} \right) \\
\end{aligned}
$$

We will further impose measurement error on these values, taking the following form:

$$
\varepsilon_i \sim \text{i.i.d. } \mathcal N \left( \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 & 0 \\ 0 & 3 \end{bmatrix} \right), \quad i = 1, \dots, 500
$$

To make clear that our true values are assumed to be fixed, we will refer to the generated $\mathbf X$ values from above in lowercase. The true values in our toy example are thus $\mathbf x_1, \dots \mathbf x_{500}$. The measured values corresponding to these observations are $\mathbf U_1, \dots \mathbf U_{500} = \mathbf x_1 + \varepsilon_1, \dots, \mathbf x_{500} + \varepsilon_{500}$.

The true $\mathbf x$ values and the measured $\mathbf U$ values are shown in Figure \@ref(fig:trueData).

```{r echo=F, warning=F}
toy <- new.env()
source("../Toys/svm_measurement_error_toy.R", toy)
attach(toy)

data <- make_base_data()
#kable(data %>% select(x1, x2) %>% add_column(error1 = 1, error2 = 3) %>% head(4), digits = 3)
```

```{r trueData, fig.cap = "True Data (left) and Measured Data (right)"}
plt1 <- plot_data_with_error(data)

noisy <- noisify_data(data)
plt2 <- plot_data_with_error(noisy)

plot_grid(
  plt1 +
    ggtitle("True Values") +
    theme(legend.position = "none"),
  plt2 +
    ggtitle("Measured Values"),
  rel_widths = c(1, 1.3)
)
```

```{r}
svm_true <- svm_metrics(data)
svm_measured <- svm_metrics(noisy)
```

Consider the case of fitting a linear SVM to the set of measured values above. We split our values into training and test sets, then use 10-fold cross-validation in the training set to determine the optimal cost parameter of $\lambda = 0.18$. The fitted model is shown in figure \@ref(fig:singleSvm), and its prediction accuracy in the test set is `r sprintf("%0.3f", svm_measured$accuracy)`. By comparison, the same exact process run against the true values instead of the measured values yields an accuracy of `r sprintf("%0.3f", svm_true$accuracy)`.

```{r singleSvm, fig.cap = "Linear SVM Fit to Measured Values"}
plot_data_with_decision_boundaries(noisy, svm_measured, alpha = 1)
```

The discrepancy in accuracy is no surprise. Clearly, the presence of measurement error will affect our ability to discern between the two classes. But remember that our test set consists of measured values. Suppose we had tested with diffrent measurements of the same underlying true values. Might we observe different accuracy? Similarly, we trained this model on measured values. Suppose we had trained the model on a different set of measurements. This would almost certainly shift our decision boundary.

This raises two fundamental questions: How widely does our decision boundary shift under the influence of measurement error? How widely does the performance of our classifier (as measured by metrics like accuracy, sensitivity, specificity, etc.) vary in the presence of measurement error?

In the case of a toy example like this, we can address these questions by taking repeated measurements of the same observations, then evaluating the spectrum of decision boundaries and performance metrics that result from repeatedly fitting classifiers. More concretely, consider taking $B$ sets of measurements:

$$
\begin{aligned}
\mathbf U^{(1)}_i &= \mathbf x_i + \varepsilon^{(1)}_i, \quad i = 1, \dots, 500 \\
& \quad \quad \vdots \\
\mathbf U^{(B)}_i &= \mathbf x_i + \varepsilon^{(B)}_i, \quad i = 1, \dots, 500
\end{aligned}
$$

From these $B$ sets of measurements, you can effectively derive Monte Carlo estimates of your preferred metrics, decision boundaries, etc. by simple observation of the distribution of the relevant quantities over the $B$ sets of measurements. Unfortunately, this method is not useful in a broader context, as we generally do not have the capacity to take large numbers of measurements on the same observations, and we do not know the true $\mathbf x_i$ values from which to simulate repeated measurements.

# Gaussian Perturbation Pipeline {#pipeline}

The method we consider for the general case is the use of Gaussian perturbation on a single set of $n$ observed measurements, $\{\mathbf U_i \}_{i = 1}^n$. A diagram is given in Figure \@ref(pipelineDiagram). In particular we generate $B$ sets of $n$ perturbed measurements, $\{\mathbf W^{(1)}_i\}, \dots, \{ \mathbf W^{(B)}_i\}$ by repeatedly adding noise to these measurements independently sampled from a normal distribution with the known measurement error variance, $\varepsilon^{(j)}_i \sim \text{i.i.d. } \mathcal N(\mathbf 0, \tau^2)$: 

$$
\begin{aligned}
\mathbf W^{(1)}_i \ | \ \mathbf U_i &= \mathbf U_i + \varepsilon^{(1)}_i, \quad i = 1, \dots, n \\
& \vdots \\
\mathbf W^{(B)}_i \ | \ \mathbf U_i &= \mathbf U_i + \varepsilon^{(B)}_i, \quad i = 1, \dots, n
\end{aligned}
$$

In contrast with the method just described in Section \@ref(motivation), the error in the perturbed measurements here is being added to measurements that already incorporate measurement error---not to the (unknown) true values directly---hence the playful use of $\mathbf W$ (double U) to denote the perturbed measurements.

![Gaussian perturbation pipeline for uncertainty quantification](../Presentation/pipeline.png){#pipelineDiagram}


Armed with these perturbed measurements, we can then apply any classifier or classifiers to each of the sets $\{ \mathbf W^{(1)}_i \}, \dots, \{\mathbf W^{(B)}_i \}$. Before doing so, we want to be careful to define a single partition of training and test observations, $\{1, \dots, n \} = I_\text{train} \cup I_\text{test}$. We will use the same training and test indices when evaluating the classification results on each of the perturbed datasets.

As we will shortly see, the distribution of a given metric observed on the set $\{ \mathbf W_i \}$ will in general provide a biased estimate of the corresponding metric on $\{ \mathbf U_i \}$. Consequently, we will use the variance of the metric as observed over the $B$ sets $\{ \mathbf W^{(j)}_i \}$ to augment the point estimate we can derive from $\{ \mathbf U_i \}$ itself.

We can also use the set of $B$ classification results to quantify the uncertainty of our individual predictions and the stability of the classification process overall. Consider the case of a binary classifier $f$ whose output for observations $i = 1, \dots, n$ takes values $f(\mathbf x) \in [0, 1]$, where $f(\mathbf x)$ represents the probability that the $\mathbf x$ belongs to class 1. Using our sets of perturbed measurements, we can use this classifier to obtain $B$ outputs for each of the $n$ observations, $f(\mathbf W^{(1)}_i), \dots, f( \mathbf W^{(B)}_i), \ i = 1, \dots, n$. In a stable algorithm, these values would be very similar for each observation $i$. In an unstable algorithm, there would be higher variance among these values.

We can use the classifications on our perturbed measurements to obtain a softened classification for each observation by simply taking the average over each of the $B$ classifications, i.e.:

$$
\hat y_i = \frac 1B \sum_{j = 1}^B f(\mathbf W^{(j)}_i)
$$

For a new measurement $\mathbf u$ (i.e., a measurement that is neither in the training nor test set), a softened classification can be obtained by generating $B$ perturbations of $\mathbf u$ using the known measurement error and averaging over these predictions.

We can use the softened classifications on our original set of $n$ observations to quantify the instability of the algorithm, $\hat I \in [0, 1]$, defined below, over all $n$ measurements. $\hat I$ can be interpreted as twice the average standard deviation of the $B$ predictions for a given observation $i$, $\{ f(\mathbf W^{(j)}_i) \}_{j = 1}^B$, averaged over the $n$ observations ($i = 1, \dots, n$). $\hat I = 0$ indicates that the algorithm is perfectly stable over the training and testing set, while larger values indicate greater instability. The greatest possible instability of $\hat I = 1$ would be attained if the predictions for each individual observation were evenly split between 0 and 1.

$$
\hat I = \frac 2n \sum_{i = 1}^n \sqrt { \frac1B \sum_{j = 1}^B (f(\mathbf W^{(j)}_i) - \hat y_i)^2 }
$$

```{r}
classifier_instab <- function(preds) {
  # preds should be a matrix of predictions with:
  # B columns for each perturbed set
  # n rows for each observation
  B <- ncol(preds)
  y <- rowMeans(preds)
  y_matrix <- matrix(rep(y, B), ncol = B)
  2 * mean(sqrt(rowMeans((preds - y_matrix)^2)))
}
```


# Application: Toy Example {#toyexample}

In our toy example before, we had observed a classification accuracy of `r sprintf("%0.3f", svm_measured$accuracy)` for an appropriately tuned linear SVM applied to one set of measured values $\{ \mathbf U_i \}_{i = 1}^{500}$. Our goal is to estimate the distribution of classifier accuracy from a set of $B = 500$ perturbed sets of measurements, $\{\mathbf W^{(1)}_i\}, \dots, \{\mathbf W^{(B)}_i\}$. Since in the contrived case of our toy example, we can take repeated measurements, we can compare the results of the method from Section \@ref(pipeline) to the results obtained from actually capturing $B$ sets of measurements, $\{\mathbf U^{(1)}_i\}, \dots, \{\mathbf U^{(B)}_i\}$.

```{r toyExampleBoxplots, cache=T, echo=F, fig.cap = "A Comparison of Estimated Accuracy Distributions"}
# simulate measurement error on one clean observation
sim_results_me <- run_simulations(data, 500, 1)

# simulate measurement error on one noisy observation
sim_results_noisy <- run_simulations(noisy, 500, 1)

delta <- svm_measured$accuracy - mean(sim_results_noisy$accuracy)

ggplot(bind_rows(
  sim_results_me %>% transmute(observed = "A: Repeated Measurements", accuracy = accuracy),
  sim_results_noisy %>% transmute(observed = "B: Perturbed Measurements", accuracy = accuracy),
  tibble(observed = "C: GPP Estimate", accuracy = sim_results_noisy$accuracy + delta)
)) +
  geom_boxplot(aes(x = observed, y = accuracy, fill = accuracy)) +
  geom_hline(yintercept = svm_measured$accuracy, color = "#4499cc", linetype = "dotted") +
  xlab(" ") +
  ylab("Accuracy")
```

Figure \@ref(fig:toyExampleBoxplots) shows the results of three different estimated distributions of classification accuracy. The leftmost distribution (A) is the estimate derived from the sets of repeated measurements, $\{\mathbf U^{(1)}_i\}, \dots, \{\mathbf U^{(B)}_i\}$. This is, in effect, the distribution we're trying to estimate. The middle distribution (B) is derived from the sets of perturbed measurements, $\{\mathbf W^{(1)}_i\}, \dots, \{\mathbf W^{(B)}_i\}$. The final distribution (C) is the result of shifting distribution B to be centered (in terms of mean) at our original observed accuracy of `r sprintf("%0.3f", svm_measured$accuracy)` (denoted by the dotted blue line). Of course, our ability to re-center such a distribution is dependent on our original point estimate. Ideally this point estimate will not lie far from its expected value.


```{r toyStability, cache = T, fig.cap = "Decision Boundaries and Softened Classifications via Perturbed Measurements"}

# caching because all_predictions is hilariously slow
preds <- all_predictions(sim_results_noisy, noisy)
instab <- classifier_instab(preds)

plot_data_with_decision_boundaries_and_soft_classifications(noisy, sim_results_noisy)

# KILL THE TOY
detach(toy)
```

Figure \@ref(fig:toyStability) depicts the decision boundaries resulting from fitting the classifier to each of the 500 sets of perturbed measurements as well as the softened classifications, $\{ \hat y_i \}_{i = 1}^{500}$, resulting from averaging predictions over these decision boundaries. The algorithm is generally stable, so most predictions are unaffected by the perturbations. Near the middle of the plot, some points are visible with softened classifications.[^softened] The calculated instability value on this dataset is $\hat I$ = `r round(instab, 4)`.

[^softened]: Note that it is also possible to start with a soft classifier and obtain further softened results by averaging over the $B$ predictions. For the sake of simplicity of illustration, a hard classifier is used here.

# Application: Astronomy Data {#astro}

```{r include = F}
train.set1 <- fread("../Data/train_set1.csv")
train.set2 <- fread("../Data/train_set2.csv")
test.set <- fread("../Data/test_set.csv")

# combine training sets
train.set <- rbind(train.set1, train.set2)

# convert class to factor
train.set$class <- as.factor(train.set$class)
test.set$class <- as.factor(test.set$class)
```

Classification method have been important tools in Astronomy for decades, and continue to be essential in studying the night sky [@kurtz1988astronomical]. With ever-increasing volumes of publicly available data, Astronomy is an area of research that is particularly well-suited for studying big data algorithms.

We acquired our data from the Sloan Digital Sky Survey (SDSS) Data Release #12. The SDSS is one of the largest astronomical surveys, scanning over a third of the night sky and recording nearly 1 billion objects to date. Each row in the data represents a single point source in the sky and is classified as either a Galaxy, Star, or Quasar. The variables we use in our application are the point spread function (PSF) magnitudes at 5 wavelengths: ultraviolet ($u$), green ($g$), red ($r$), near infrared ($i$), and infrared ($z$). For each observation, we also have the known measurement error at each wavelength. We will use these errors as the standard deviation for our Gaussian perturbation. The features used for classification are the differences of the magnitudes, called colors: $u-g, g-r, r-i, and i-z$.

To clean the data, objects with missing values and incorrect measurements were removed. To simplify the problem, we exclude quasars in our example as they make up a very small portion of the data. Our final dataset contains 234,972 objects and 4 features.

The classification task is to predict the class (galaxy or star) of an object based on the observed color values.

## Exploratory Analysis

From Table \@ref(tab:astroClassProp), we notice that there is slight class imbalance in the data. Various techniques can be used to address the imbalance, such as upsampling, downsampling, and SMOTE. However, we do not demonstrate them here as improving prediction is not the main purpose of the project.

```{r astroClassProp, echo = F}
# create class proportion table for full set
class.tbl <- prop.table(table(c(train.set$class, test.set$class)))
names(class.tbl) <- c("Galaxy", "Star")

# print table
class.tbl %>% t %>% kable("latex", caption = "Class proportions of Astronomy dataset", booktabs = T)
```

```{r colorPlot, cache = T, fig.height = 2, fig.cap = "Marginal distributions of the Astronomy data", echo = F}
# pivot longer full dataset
full.set.long <- rbind(train.set, test.set) %>% gather(key = "color", value = "value", u_g, g_r, r_i, i_z)

# plot colors by class
ggplot(full.set.long, aes(x = value, fill = class)) +
  geom_density(col = NA) +
  facet_grid(.~ color) +
  xlim(-1, 4)
```

In Figure \@ref(fig:colorPlot), we show the marginal distributions of each feature, colored by the object class. The difference in distribution observed between the two classes indicates that there is class separation in the chosen feature space. We hope that classification algorithms can distinguish between the two classes using these features.

## Astro Perturbation Results: Confidence intervals

We created 500 perturbed sets using the original dataset and the known measurement error on each observation. Note that this example differs from the toy example in that each observation has its own measurement error. The standard deviation used to perturb each observation varies.

To illustrate how one might compare stability across multiple classification algorithms, we applied 3 algorithms: logistic regression, Naive Bayes, and Random Forest, which are all well-suited for binary classification. For each classifier, we calculated the 95% perturbation confidence interval for 5 commonly used performance metrics and the stability score as defined in Section 3. The resulting confidence interval for each classifier is presented in Table \@ref(tab:astroCI). The instability score for each classifier is also shown in Table \@ref(tab:astroStab). 

```{r, include = F}
# reference GLM
load("../Data/reference_glm_measures.RData")

# reference NB
load("../Data/reference_nb_measures.RData")

# reference RF
load("../Data/reference_rf_measures.RData")
```

```{r astroCI, echo = F}
# perturbed glm
p.glm.meas <- fread("../Data/glm_measures500.csv")
p.glm.meas.sd <- apply(p.glm.meas, 2, sd)

p.glm.lower <- (glm.meas - 2*p.glm.meas.sd) %>% round(4)
p.glm.upper <- (glm.meas + 2*p.glm.meas.sd) %>% round(4)

# perturbed nb
p.nb.meas <- fread("../Data/nb_measures500.csv")
p.nb.meas.sd <- apply(p.nb.meas, 2, sd)

p.nb.lower <- (nb.meas - 2*p.nb.meas.sd) %>% round(4)
p.nb.upper <- (nb.meas + 2*p.nb.meas.sd) %>% round(4)

# perturbed RF
p.rf.meas <- fread("../Data/measures500.csv")
p.rf.meas.sd <- apply(p.rf.meas, 2, sd)

p.rf.lower <- (rf.meas - 2*p.rf.meas.sd) %>% round(4)
p.rf.upper <- (rf.meas + 2*p.rf.meas.sd) %>% round(4)


glm.tbl <- sprintf("(%0.4f, %0.4f)", p.glm.lower, pmin(1, p.glm.upper)) %>%
  matrix(nrow = length(glm.meas), dimnames = list(names(glm.meas), "95% CI (Logit)"))

nb.tbl <- sprintf("(%0.4f, %0.4f)", p.nb.lower, pmin(1, p.nb.upper)) %>%
  matrix(nrow = length(nb.meas), dimnames = list(names(nb.meas), "95% CI (NB)"))

rf.tbl <- sprintf("(%0.4f, %0.4f)", p.rf.lower, pmin(1, p.rf.upper)) %>%
  matrix(nrow = length(rf.meas), dimnames = list(names(rf.meas), "95% CI (RF)"))

cbind(glm.tbl, nb.tbl, rf.tbl) %>%
  kable("latex", caption = "95\\% confidence intervals from Gaussian perturbation", booktabs = T) %>%
  kable_styling(font_size = 10)
```


```{r astroStab, echo = F}
glm.instab <- 0.09204126*2
nb.instab <- 0.02805608*2
rf.instab <- 0.08936165*2

matrix(c(glm.instab, nb.instab, rf.instab), dimnames = list(c("Logit", "Naive Bayes", "Random Forest"),"Instability Score")) %>%
  kable("latex", booktabs = T, caption = "Stability of classifiers")
```

Notice the relationship between the interval widths and the stability score. Naive Bayes had the narrowest intervals and lowest instability score, indicating that our Naive Bayes classifier is more stable compared with logistic regression and Random Forest. This makes sense since Naive Bayes is, theoretically, a high bias and low variance classifier. The results of Naive Bayes are robust to small perturbations in the training data.

Logistic regression and random forest received a similar instability score, in which case we can choose the algorithm that simply performs better on new unseen data. However, we run into trouble when comparing their confidence intervals. All five of the performance metrics are bounded on the unit interval. Since random forest performs very well on this data, the confidence intervals exceed 1. For this reason, we truncated the confidence intervals at 1 in table \@ref(tab:astroCI). Therefore, the method we used for calculating confidence intervals may not make sense if the performance measures fall near 0 and 1. Further consideration is needed to better handle these confidence intervals. One potential solution is to choose a distribution other than Gaussian to calculate the intervals and somehow using the variation obtained from perturbing.


# Discussion

As you can see, our method lacks theoretical justification, but it is otherwise incredible. If you give it a try, I think you'll find this method is worthy of an A+.

## Author Contributions

**TODO:** (I believe this is required.) Sarah did the astro stuff. Jonathan did the toy stuff. Everything else was shared.

# Bibliography

<!-- pls don't delete yet. I'll get to it, but I need to store it somewhere for now -->

<!--
JH:

I hope you don't mind, but I moved the reference into the bibliography.

To add more references, just search in Google scholar, copy and paste the BibTeX entry (see https://libguides.usask.ca/c.php?g=218034&p=1445751) into sources.bib, and then reference the work in text here. For a parenthetical citation (e.g., "some fact (Whatever 2020)") , use `[@whatever2020lol]`. For a non-parenthetical citation (e.g., "according to Whatever 2020"), drop the square brackets.

If you want works in sources.bib to appear in the bibliography even if not cited in the paper, add `nocite: "@*"` to the YAML header. Then everything will appear.

-->
