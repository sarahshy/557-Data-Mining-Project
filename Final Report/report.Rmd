---
title: "STAT 557 Final Project Report"
author: "Sarah Shy, Jonathan Hehir"
date: "Spring 2020"
geometry: margin=1in
fontfamily: charter
fontsize: 12pt
bibliography: sources.bib
csl: american-statistical-association.csl
#classoption:
#  - twocolumn
numbersections: true
output:
  bookdown::pdf_document2:
    toc: true
    toc_depth: 1
    template: null
abstract: |
  This is a test. This is a test. This is a test. This is a test. This is a test.
  This is a test. This is a test. This is a test. This is a test. This is a test.
---


\setlength\abovedisplayskip{0pt}

```{r global_options, include = FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.width = 6,
                      fig.height = 3)
```

```{r}
# Front matter
library(cowplot)
library(tidyverse)
library(knitr)
library(data.table)
library(knitr)
library(kableExtra)
set.seed(3)
theme_set(theme_bw(base_size = 10))
```

<!--
Game Plan (to be deeleeted):

* Abstract
* Intro:
    + what is measurement error and why we should use it for uncertainty quantification
    + what is gaussian perturbation and what we get from it?
        - uncertainty quantification (important if we observe only one noisy instance, ideally measure many times)
        - stability (important for reproducibility, trustworthiness, more bs like that)
        - soft classification (for classifiers that don't have probabilities)
* Formal Gaussian perturbation pipeline
* Toy example for illustration
* Astro data 
* Discussion:
    + Recap what we have
* Limitations
* Future Directions and Related Work
* Bib

-->

# Introduction

Measurement error is the difference between a measured quantity and its true value [@dodge2006oxford]. Generally, several measurements on the same quantity on the same subject will not be the same. This variation may arise due to variation in the measurement process, the inherent variation in the subject, or both [@bland1996statistics]. In this report, we focus on the former, assuming that the true value of a quantity of interest is fixed. Furthermore, we rule out systematic error, which may arise with inconsistent calibration, and focus only on random error.

In many statistical scenarios, we assume that our data are perfect measurements of a quantity of interest. This often false assumption can lead to bias in parameter estimation for statistical models, loss of power for detecting interesting variable relationships, and masking of features of the data [@carroll2006measurement]. It is therefore crutial for us to investigate how measurement error influences modeling and to quantify uncertainty in statistical inference and estimation.

In many contexts, we know the variation of our measurement error. This is particularly true in various scientific and engineering settings, where the random error of a measuring device can be measured by the manufacturer or the scientist. In other settings, when the measurement error is unknown, but one can take repeated measurements on each subject, we can estimate the within-subject variation [@bland1996statistics]. In this paper, we focus on the use of known measurement error, although the results would apply in a similar fashion to estimated measurement error. 

The goal of this paper is to assess the stability of a classification algorithm and quantify uncertainty in classification results in the presence of measurement error. While existing research on uncertainty quantification is largely algorithm-specific, our work explores a general pipeline for quantifying uncertainty with any classifier. This would allow us to better compare uncertainty, stability, and overall performance across multiple classifiers. This report summarizes our exploratory work in this area of research.

We begin by motivating the subject with a toy example in Section \@ref(motivation). In Section \@ref(pipeline), we introduce the Gaussian perturbation pipeline for stability and uncertainty quantification. In Section \@ref(toyexample), we apply the pipeline to the previously simulated toy data. In Section \@ref(astro), we present a real-world application to Astronomy data. Finally, we discuss the results and future direction in Section \@ref(discussion).

# Motivation {#motivation}

In order to illustrate the relevant issues that arise in the presence of measurement error, we begin by introducing a toy example. We will use a set of data representing true values belonging to two classes by sampling 250 points each from two bivariate normal distributions:

$$
\begin{aligned}
\mathbf X_{1}, \dots, \mathbf X_{250} &\sim \text{i.i.d. } \mathcal N \left( \begin{bmatrix} 1 \\ 2 \end{bmatrix}, \begin{bmatrix} 3 & -0.5 \\ -0.5 & 2 \end{bmatrix} \right) \\
\mathbf X_{251}, \dots, \mathbf X_{500} &\sim \text{i.i.d. }\mathcal N \left( \begin{bmatrix} -1 \\ -1 \end{bmatrix}, \begin{bmatrix} 4 & -1 \\ -1 & 1 \end{bmatrix} \right) \\
\end{aligned}
$$

We will further impose measurement error on these values, taking the following form:

$$
\varepsilon_i \sim \text{i.i.d. } \mathcal N \left( \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 & 0 \\ 0 & 3 \end{bmatrix} \right), \quad i = 1, \dots, 500
$$

To make clear that our true values are assumed to be fixed, we will refer to the generated $\mathbf X$ values from above in lowercase. The true values in our toy example are thus $\mathbf x_1, \dots \mathbf x_{500}$. The measured values corresponding to these observations are $\mathbf U_1, \dots \mathbf U_{500} = \mathbf x_1 + \varepsilon_1, \dots, \mathbf x_{500} + \varepsilon_{500}$.

The true $\mathbf x$ values and the measured $\mathbf U$ values are shown in Figure \@ref(fig:trueData).

```{r echo=F, warning=F}
toy <- new.env()
source("../Toys/svm_measurement_error_toy.R", toy)
attach(toy)

data <- make_base_data()
#kable(data %>% select(x1, x2) %>% add_column(error1 = 1, error2 = 3) %>% head(4), digits = 3)
```

```{r trueData, fig.cap = "True Data (left) and Measured Data (right)"}
plt1 <- plot_data_with_error(data)

noisy <- noisify_data(data)
plt2 <- plot_data_with_error(noisy)

plot_grid(
  plt1 +
    ggtitle("True Values") +
    theme(legend.position = "none"),
  plt2 +
    ggtitle("Measured Values"),
  rel_widths = c(1, 1.3)
)
```

```{r}
svm_true <- svm_metrics(data)
svm_measured <- svm_metrics(noisy)
```

Consider the case of fitting a linear SVM to the set of measured values above. We split our values into training and test sets, then use 10-fold cross-validation in the training set to determine the optimal cost parameter of $\lambda = 0.18$. The fitted model is shown in figure \@ref(fig:singleSvm), and its prediction accuracy in the test set is `r sprintf("%0.3f", svm_measured$accuracy)`. By comparison, the same exact process run against the true values instead of the measured values yields an accuracy of `r sprintf("%0.3f", svm_true$accuracy)`.

```{r singleSvm, fig.cap = "Linear SVM Fit to Measured Values"}
plot_data_with_decision_boundaries(noisy, svm_measured, alpha = 1)
```

The discrepancy in accuracy is no surprise. Clearly, the presence of measurement error will affect our ability to discern between the two classes. But remember that our test set consists of measured values. Suppose we had tested with diffrent measurements of the same underlying true values. Might we observe different accuracy? Similarly, we trained this model on measured values. Suppose we had trained the model on a different set of measurements. This would almost certainly shift our decision boundary.

This raises two fundamental questions: How widely does our decision boundary shift under the influence of measurement error? How widely does the performance of our classifier (as measured by metrics like accuracy, sensitivity, specificity, etc.) vary in the presence of measurement error?

In the case of a toy example like this, we can address these questions by taking repeated measurements of the same observations, then evaluating the spectrum of decision boundaries and performance metrics that result from repeatedly fitting classifiers. More concretely, consider taking $B$ sets of measurements:

$$
\begin{aligned}
\mathbf U^{(1)}_i &= \mathbf x_i + \varepsilon^{(1)}_i, \quad i = 1, \dots, 500 \\
& \quad \quad \vdots \\
\mathbf U^{(B)}_i &= \mathbf x_i + \varepsilon^{(B)}_i, \quad i = 1, \dots, 500
\end{aligned}
$$

From these $B$ sets of measurements, you can effectively derive Monte Carlo estimates of your preferred metrics, decision boundaries, etc. by simple observation of the distribution of the relevant quantities over the $B$ sets of measurements. Unfortunately, this method is not useful in a broader context, as we generally do not have the capacity to take large numbers of measurements on the same observations, and we do not know the true $\mathbf x_i$ values from which to simulate repeated measurements.

# Gaussian Perturbation Pipeline {#pipeline}

The method we consider for the general case is the use of Gaussian perturbation on a single set of $n$ observed measurements, $\{\mathbf U_i \}_{i = 1}^n$. A diagram is given in Figure \@ref(pipelineDiagram). In particular we generate $B$ sets of $n$ perturbed measurements, $\{\mathbf W^{(1)}_i\}, \dots, \{ \mathbf W^{(B)}_i\}$ by repeatedly adding noise to these measurements independently sampled from a normal distribution with the known measurement error variance, $\varepsilon^{(j)}_i \sim \text{i.i.d. } \mathcal N(\mathbf 0, \tau^2)$: 

$$
\begin{aligned}
\mathbf W^{(1)}_i \ | \ \mathbf U_i &= \mathbf U_i + \varepsilon^{(1)}_i, \quad i = 1, \dots, n \\
& \vdots \\
\mathbf W^{(B)}_i \ | \ \mathbf U_i &= \mathbf U_i + \varepsilon^{(B)}_i, \quad i = 1, \dots, n
\end{aligned}
$$

In contrast with the method just described in Section \@ref(motivation), the error in the perturbed measurements here is being added to measurements that already incorporate measurement error---not to the (unknown) true values directly---hence the playful use of $\mathbf W$ (double U) to denote the perturbed measurements.

![Gaussian perturbation pipeline for uncertainty quantification](../Presentation/pipeline.png){#pipelineDiagram}


Armed with these perturbed measurements, we can then apply any classifier or classifiers to each of the sets $\{ \mathbf W^{(1)}_i \}, \dots, \{\mathbf W^{(B)}_i \}$. Before doing so, we want to be careful to define a single partition of training and test observations, $\{1, \dots, n \} = I_\text{train} \cup I_\text{test}$. We will use the same training and test indices when evaluating the classification results on each of the perturbed datasets.

As we will shortly see, the distribution of a given metric observed on the set $\{ \mathbf W_i \}$ will in general provide a biased estimate of the corresponding metric on $\{ \mathbf U_i \}$. Consequently, we will use the variance of the metric as observed over the $B$ sets $\{ \mathbf W^{(j)}_i \}$ to augment the point estimate we can derive from $\{ \mathbf U_i \}$ itself.

**TODO:* Add more here?

# Application: Toy Example {#toyexample}

In our toy example before, we had observed a classification accuracy of `r sprintf("%0.3f", svm_measured$accuracy)` for an appropriately tuned linear SVM applied to one set of measured values $\{ \mathbf U_i \}_{i = 1}^{500}$. Our goal is to estimate the distribution of classifier accuracy from a set of $B = 500$ perturbed sets of measurements, $\{\mathbf W^{(1)}_i\}, \dots, \{\mathbf W^{(B)}_i\}$. Since in the contrived case of our toy example, we can take repeated measurements, we can compare the results of the method from Section \@ref(pipeline) to the results obtained from actually capturing $B$ sets of measurements, $\{\mathbf U^{(1)}_i\}, \dots, \{\mathbf U^{(B)}_i\}$.

```{r toyExampleBoxplots, cache=T, echo=F, fig.cap = "A Comparison of Estimated Accuracy Distributions"}
# simulate measurement error on one clean observation
sim_results_me <- run_simulations(data, 500, 1)

# simulate measurement error on one noisy observation
sim_results_noisy <- run_simulations(noisy, 500, 1)

delta <- svm_measured$accuracy - mean(sim_results_noisy$accuracy)

ggplot(bind_rows(
  sim_results_me %>% transmute(observed = "A: Repeated Measurements", accuracy = accuracy),
  sim_results_noisy %>% transmute(observed = "B: Perturbed Measurements", accuracy = accuracy),
  tibble(observed = "C: GPP Estimate", accuracy = sim_results_noisy$accuracy + delta)
)) +
  geom_boxplot(aes(x = observed, y = accuracy, fill = accuracy)) +
  geom_hline(yintercept = svm_measured$accuracy, color = "#4499cc", linetype = "dotted") +
  xlab(" ") +
  ylab("Accuracy")
```

Figure \@ref(fig:toyExampleBoxplots) shows the results of three different estimated distributions of classification accuracy. The leftmost distribution (A) is the estimate derived from the sets of repeated measurements, $\{\mathbf U^{(1)}_i\}, \dots, \{\mathbf U^{(B)}_i\}$. This is, in effect, the distribution we're trying to estimate. The middle distribution (B) is derived from the sets of perturbed measurements, $\{\mathbf W^{(1)}_i\}, \dots, \{\mathbf W^{(B)}_i\}$. The final distribution (C) is the result of shifting distribution B to be centered (in terms of mean) at our original observed accuracy of `r sprintf("%0.3f", svm_measured$accuracy)` (denoted by the dotted blue line). Of course, our ability to re-center such a distribution is dependent on our original point estimate. Ideally this point estimate will not lie far from its expected value.

**TODO: BUT WAIT! THERE'S MORE! LET'S DO BOUNDARY STABILITY!**

```{r echo=F}
plot_data_with_decision_boundaries(noisy, sim_results_noisy)
```

Averaging predictions over those 500 decision boundaries yields a softened classifier:

```{r}
classified <- soft_classify_set(sim_results_noisy, noisy)

ggplot(classified) +
  geom_point(aes(x = x1, y = x2, color = p)) +
  scale_color_gradient(low = "#cc0011", high = "#223399") +
  xlim(-10, 10) +
  ylim(-10, 10)

# KILL THE TOY
detach(toy)
```

# Application: Astronomy Data {#astro}

Classification method have been important tools in Astronomy for decades, and continue to be essential in studying the night sky [@kurtz1988astronomical]. With ever-increasing volumes of publicly available data, Astronomy is an area of research that is particularly well-suited for studying big data algorithms.

We acquired our data from the Sloan Digital Sky Survey (SDSS) Data Release #12. The SDSS is one of the largest astronomical surveys, scanning over a third of the night sky and recording nearly 1 billion objects to date.

## Astro Results: Confidence intervals


### Logistic Regression

```{r, include = F}
# reference RF
load("../Data/reference_glm_measures.RData")
```

```{r, echo = F}
# perturbed glm
meas <- fread("../Data/glm_measures500.csv")
meas.sd <- apply(meas, 2, sd)

lower <- (glm.meas - 2*meas.sd) %>% round(4)
upper <- (glm.meas + 2*meas.sd) %>% round(4)

sprintf("(%0.4f, %0.4f)", lower, pmin(1, upper)) %>%
  matrix(nrow = length(glm.meas), dimnames = list(names(glm.meas), "95% CI")) %>%
  kable("latex", booktabs = T) %>%
  kable_styling(font_size = 10)

#quantile(meas$AUC, 0.025)
#quantile(meas$AUC, 0.975)
```

### Naive Bayes

```{r, include = F}
# reference RF
load("../Data/reference_nb_measures.RData")
```

```{r, echo = F}
# perturbed nb
meas <- fread("../Data/nb_measures500.csv")
meas.sd <- apply(meas, 2, sd)

lower <- (nb.meas - 2*meas.sd) %>% round(4)
upper <- (nb.meas + 2*meas.sd) %>% round(4)

sprintf("(%0.4f, %0.4f)", lower, pmin(1, upper)) %>%
  matrix(nrow = length(nb.meas), dimnames = list(names(nb.meas), "95% CI")) %>%
  kable("latex", booktabs = T) %>%
  kable_styling(font_size = 10)

#quantile(meas$AUC, 0.025)
#quantile(meas$AUC, 0.975)
```

### Random Forest

```{r, include = F}
# reference RF
load("../Data/reference_rf_measures.RData")
```

```{r, echo = F}
# perturbed RF
meas <- fread("../Data/measures500.csv")
meas.sd <- apply(meas, 2, sd)

lower <- (rf.meas - 2*meas.sd) %>% round(4)
upper <- (rf.meas + 2*meas.sd) %>% round(4)

sprintf("(%0.4f, %0.4f)", lower, pmin(1, upper)) %>%
  matrix(nrow = length(rf.meas), dimnames = list(names(rf.meas), "95% CI")) %>%
  kable("latex", booktabs = T) %>%
  kable_styling(font_size = 10)

#quantile(meas$AUC, 0.025)
#quantile(meas$AUC, 0.975)
```

Observations: Naive Bayes has the tightest intervals. This makes sense since Naive Bayes is a high bias, low variance classifier! We see it's pretty robust to perturbations.

<!--

JH: One more potentially annoying question here:

Can we apply this on more than just random forest for the sake of the paper? Like we can do RF, SVM, and logistic or something and then compare them? We're arguing in the intro that we want to be able to use this to compare classifiers, but right now, we're not actually comparing any classifiers.

-->

# Discussion

As you can see, our method lacks theoretical justification, but it is otherwise incredible. If you give it a try, I think you'll find this method is worthy of an A+.

## Author Contributions

**TODO:** (I believe this is required.) Sarah did the astro stuff. Jonathan did the toy stuff. Everything else was shared.

# Bibliography

<!-- pls don't delete yet. I'll get to it, but I need to store it somewhere for now -->

<!--
JH:

I hope you don't mind, but I moved the reference into the bibliography.

To add more references, just search in Google scholar, copy and paste the BibTeX entry (see https://libguides.usask.ca/c.php?g=218034&p=1445751) into sources.bib, and then reference the work in text here. For a parenthetical citation (e.g., "some fact (Whatever 2020)") , use `[@whatever2020lol]`. For a non-parenthetical citation (e.g., "according to Whatever 2020"), drop the square brackets.

If you want works in sources.bib to appear in the bibliography even if not cited in the paper, add `nocite: "@*"` to the YAML header. Then everything will appear.

-->
