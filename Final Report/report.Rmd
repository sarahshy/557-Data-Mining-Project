---
title: "STAT 557 Final Project Report"
author: "Sarah Shy, Jonathan Hehir"
date: "Spring 2020"
geometry: margin=1in
fontfamily: charter
fontsize: 12pt
bibliography: sources.bib
csl: american-statistical-association.csl
#classoption:
#  - twocolumn
numbersections: true
output:
  bookdown::pdf_document2:
    toc: true
    toc_depth: 1
    template: null
abstract: |
  This is a test. This is a test. This is a test. This is a test. This is a test.
  This is a test. This is a test. This is a test. This is a test. This is a test.
---


\setlength\abovedisplayskip{0pt}

```{r global_options, include = FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.width = 6,
                      fig.height = 3)
```

```{r}
# Front matter
library(cowplot)
library(tidyverse)
library(knitr)
```

<!--
Game Plan (to be deeleeted):

* Abstract
* Intro:
    + what is measurement error and why we should use it for uncertainty quantification
    + what is gaussian perturbation and what we get from it?
        - uncertainty quantification (important if we observe only one noisy instance, ideally measure many times)
        - stability (important for reproducibility, trustworthiness, more bs like that)
        - soft classification (for classifiers that don't have probabilities)
* Formal Gaussian perturbation pipeline
* Toy example for illustration
* Astro data 
* Discussion:
    + Recap what we have
* Limitations
* Future Directions and Related Work
* Bib

-->

# Introduction

Measurement error is the difference between a measured quantity and its true value [@dodge2006oxford]. Generally, several measurements on the same quantity on the same subject will not be the same. This variation may arise due to variation in the measurement process, the inherent variation in the subject, or both [@bland1996statistics]. In this report, we focus on the former, assuming that the true value of a quantity of interest is fixed. Furthermore, we rule out systematic error, which may arise with inconsistent calibration, and focus only on random error.

In many statistical scenarios, we assume that our data are perfect measurements of a quantity of interest. This often false assumption can lead to bias in parameter estimation for statistical models, loss of power for detecting interesting variable relationships, and masking of features of the data [@carroll2006measurement]. It is therefore crutial for us to investigate how measurement error influences modeling and to quantify uncertainty in statistical inference and estimation.

In many contexts, we know the variation of our measurement error. This is particularly true in various scientific and engineering settings, where the random error of a measuring device can be measured by the manufacturer or the scientist. In other settings, when the measurement error is unknown, but one can take repeated measurements on each subject, we can estimate the within-subject variation [@bland1996statistics]. In this paper, we focus on the use of known measurement error, although the results would apply in a similar fashion to estimated measurement error. 

The goal of this paper is to assess the stability of a classification algorithm and quantify uncertainty in classification results in the presence of measurement error. While existing research on uncertainty quantification is largely algorithm-specific, our work explores a general pipeline for quantifying uncertainty with any classifier. This would allow us to better compare uncertainty, stability, and overall performance across multiple classifiers. This report summarizes our exploratory work in this area of research.

We begin by motivating the subject with a toy example in Section \@ref(motivation). In Section \@ref(pipeline), we introduce the Gaussian perturbation pipeline for stability and uncertainty quantification. In Section \@ref(toyexample), we apply the pipeline to the previously simulated toy data. In Section \@ref(astro), we present a real-world application to Astronomy data. Finally, we discuss the results and future direction in Section \@ref(discussion).

# Motivation {#motivation}

In order to illustrate the relevant issues that arise in the presence of measurement error, we begin by introducing a toy example. We will use a set of data representing true values belonging to two classes by sampling 250 points each from two bivariate normal distributions:

$$
\begin{aligned}
\mathbf X_{1}, \dots, \mathbf X_{250} &\sim \text{i.i.d. } \mathcal N \left( \begin{bmatrix} 1 \\ 2 \end{bmatrix}, \begin{bmatrix} 3 & -0.5 \\ -0.5 & 2 \end{bmatrix} \right) \\
\mathbf X_{251}, \dots, \mathbf X_{500} &\sim \text{i.i.d. }\mathcal N \left( \begin{bmatrix} -1 \\ -1 \end{bmatrix}, \begin{bmatrix} 4 & -1 \\ -1 & 1 \end{bmatrix} \right) \\
\end{aligned}
$$

We will further impose measurement error on these values, taking the following form:

$$
\varepsilon_i \sim \text{i.i.d. } \mathcal N \left( \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 & 0 \\ 0 & 3 \end{bmatrix} \right), \quad i = 1, \dots, 500
$$

To make clear that our true values are assumed to be fixed, we will refer to the generated $\mathbf X$ values from above in lowercase. The true values in our toy example are thus $\mathbf x_1, \dots \mathbf x_{500}$. The measured values corresponding to these observations are $\mathbf U_1, \dots \mathbf U_{500} = \mathbf x_1 + \varepsilon_1, \dots, \mathbf x_{500} + \varepsilon_{500}$.

The true $\mathbf x$ values and the measured $\mathbf U$ values are shown in Figure \@ref(fig:trueData).

```{r echo=F, warning=F}
toy <- new.env()
source("../Toys/svm_measurement_error_toy.R", toy)
attach(toy)

data <- make_base_data()
#kable(data %>% select(x1, x2) %>% add_column(error1 = 1, error2 = 3) %>% head(4), digits = 3)
```

```{r trueData, fig.cap = "True Data (left) and Measured Data (right)"}
plt1 <- plot_data_with_error(data)

noisy <- noisify_data(data)
plt2 <- plot_data_with_error(noisy)

plot_grid(
  plt1 +
    ggtitle("True Values") +
    theme(legend.position = "none"),
  plt2 +
    ggtitle("Measured Values"),
  rel_widths = c(1, 1.3)
)
```

Clearly, the presence of measurement error here will affect our ability to discern between the two classes. What was a fairly separable set of classes is made more murky by the measurement error. In terms of classifier accuracy, we can reasonably expect any classifier to suffer here.

<!--

TODO:

My plan at the moment is to plot the single decision boundary resulting from a linear SVM fit to the measured values. Then describe the accuracy of this vs. against the true values. (Of course, it's worse!) Then I will pose the question of what if we trained against different measurements on the same data? What if we tested against different measurements of the same data? This is the source of variation and instability that we're concerned about.

From here, we can introduce the pipeline.

-->

# Gaussian Perturbation Pipeline {#pipeline}

![Gaussian perturbation pipeline for uncertainty quantification](../Presentation/pipeline.png)

# Application: Toy Example {#toyexample}

<!--

Then we should return to the toy example. Showing box plots, etc. This is hardly theoretical justification, but it's something!

Once we're sick of toys, we can move on to stars and stuff.

-->

## Toy Example: Set up

## Toy Example: Linear Classifier on True Data and on Noisy Data

Fitting a linear SVM (tuned using 10-fold CV) to the above yields the following accuracies:

```{r include=F}
d_acc <- 100*svm_metrics(data)$accuracy
n_acc <- 100*svm_metrics(noisy)$accuracy

kable(tribble(~Dataset, ~Accuracy,
              "True", d_acc,
              "Measured", n_acc),
      "latex", digits = 4)
```

As expected, the linear classifier can better separate the classes in the true data. Some blah blah on this? In reality, we only have access to the noisy data. Can we trust this accuracy estimate having only observed one set of noisy measurements?

## Toy Example: 500 measurements

Using the same original dataset, we simulate taking noisy measurements and fitting the SVM 500 times to obtain the following distribution of classifier accuracy:

```{r cache=T, echo=F}
# simulate measurement error on one clean observation
sim_results_me <- run_simulations(data, 500, 1)

# simulate measurement error on one noisy observation
sim_results_noisy <- run_simulations(noisy, 500, 1)

#ggplot(sim_results_me) + geom_histogram(aes(x = accuracy), binwidth = 0.01, color = #"#000000", fill= "#ffaa33", size = .25) +
#  xlab("Accuracy") +
#  ylab("Frequency") +
#  theme_bw(base_size = 8)

ggplot(sim_results_me) + geom_boxplot(aes(y = accuracy), fill = "#ffaa33", size = 0.25) +
  ylab("Accuracy") +
  coord_flip() +
  theme_bw(base_size = 8) +
  theme(axis.text.y = element_blank())

detach(toy)
```

That is, we might observe this distribution if we could take 500 sets of measurements.

## Toy Example: Estimating Variance Without the True Values

```{r echo=F}
attach(toy)

ggplot(bind_rows(
  sim_results_me %>% add_column(observed = "500 Sets of Measurements"),
  sim_results_noisy %>% add_column(observed = "500 Perturbations of 1 Set of Measurements")
)) +
  geom_boxplot(aes(x = observed, y = accuracy), fill = c("#999999", "#ffaa33")) +
  xlab(" ") +
  ylab("Accuracy") +
  coord_flip() +
  theme_bw(base_size = 8)
```

- The standard deviation when taking 500 sets of measurements (what we want to estimate) is `r sprintf("%0.5f", sd(sim_results_me$accuracy))`.
- The standard deviation when performing Gaussian perturbation on one set of measurements is `r sprintf("%0.5f", sd(sim_results_noisy$accuracy))`.

## Toy Example: A Decision Boundary for Each Perturbed Dataset

```{r echo=F}
plot_data_with_decision_boundaries(noisy, sim_results_noisy)
```

## Toy Example: Soft Classification

Averaging predictions over those 500 decision boundaries yields a softened classifier:

```{r}
classified <- soft_classify_set(sim_results_noisy, noisy)

ggplot(classified) +
  geom_point(aes(x = x1, y = x2, color = p)) +
  scale_color_gradient(low = "#cc0011", high = "#223399") +
  xlim(-10, 10) +
  ylim(-10, 10)

# KILL THE TOY
detach(toy)
```

# Application: Astronomy Data {#astro}

Classification method have been important tools in Astronomy for decades, and remain essential in studying the night sky [@kurtz1988astronomical]. 

<!--

JH: One more potentially annoying question here:

Can we apply this on more than just random forest for the sake of the paper? Like we can do RF, SVM, and logistic or something and then compare them? We're arguing in the intro that we want to be able to use this to compare classifiers, but right now, we're not actually comparing any classifiers.

-->

# Discussion

# Bibliography

<!-- pls don't delete yet. I'll get to it, but I need to store it somewhere for now -->

<!--
JH:

I hope you don't mind, but I moved the reference into the bibliography.

To add more references, just search in Google scholar, copy and paste the BibTeX entry (see https://libguides.usask.ca/c.php?g=218034&p=1445751) into sources.bib, and then reference the work in text here. For a parenthetical citation (e.g., "some fact (Whatever 2020)") , use `[@whatever2020lol]`. For a non-parenthetical citation (e.g., "according to Whatever 2020"), drop the square brackets.

If you want works in sources.bib to appear in the bibliography even if not cited in the paper, add `nocite: "@*"` to the YAML header. Then everything will appear.

-->
