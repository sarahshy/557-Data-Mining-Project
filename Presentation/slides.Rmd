---
title: "Classification with Measurement Error"
author: "Sarah Shy, Jonathan Hehir"
date: "April 21, 2020"
bibliography: sources.bib
csl: american-statistical-association.csl
output: beamer_presentation

# For fancy beamer theme, we can use this instead of the `output` line above.
# But it requires having the theme installed,
# and I fear we could have cross-platform issues with LaTeX rendering.
# Skipping for now, but we can play with it.
#
# output:
#  beamer_presentation:
#    theme: metropolis
#    latex_engine: xelatex
---

```{r include=FALSE}
library(tidyverse)
library(mvtnorm)
library(knitr)
set.seed(3)
```

(this slide is blank)

# Introduction

## What is measurement error?

**Measurement error** is the difference between a measured quantity and its true value.

. . .

In many statistical scenarios, we assume that our variables are perfect measurements from some sample.

. . .

This is often **false**. In many cases, we know our measurements are inaccurate.

. . .

In especially nice cases, we know **how inaccurate** they are.

## Applications of Measurement Error

Situations where we know our measurement error include:

- Astronomy data
  - We'll come back to this.
- Privacy applications
  - Intentionally adding noise to observations is a common method for achieving privacy. We choose the amount of noise to add.
- Various scientific and engineering settings

## Toy Example: Setup

Mostly separable classes. No measurement error.

$$
\begin{aligned}
\mathbf X_{1}, \dots, \mathbf X_{250} &\sim \text{i.i.d. } \mathcal N \left( \begin{bmatrix} 1 \\ 2 \end{bmatrix}, \begin{bmatrix} 3 & -0.5 \\ -0.5 & 2 \end{bmatrix} \right) \\
\mathbf X_{251}, \dots, \mathbf X_{500} &\sim \text{i.i.d. }\mathcal N \left( \begin{bmatrix} -1 \\ -1 \end{bmatrix}, \begin{bmatrix} 4 & -1 \\ -1 & 1 \end{bmatrix} \right) \\
\varepsilon &\sim \text{i.i.d. } \mathcal N \left( \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 & 0 \\ 0 & 3 \end{bmatrix} \right)
\end{aligned}
$$

```{r echo=F, warning=F}
toy <- new.env()
source("../Toys/svm_measurement_error_toy.R", toy)
attach(toy)

data <- make_base_data()
#kable(data %>% select(x1, x2) %>% add_column(error1 = 1, error2 = 3) %>% head(4), digits = 3)
```

## Toy Example: The Clean Dataset

This is the clean data (what we wouldn't actually observe):

```{r echo=F, warning=F}
plot_data_with_error(data)
```

---

## Toy Example: Noisy Dataset

This is the noisy data (what we would actually observe):

```{r echo=F}
noisy <- noisify_data(data)
plot_data_with_error(noisy)
```

## Toy Example: Classifier Performance

Fitting a basic linear SVM to the above yields the following accuracies:

```{r include=F}
d_acc <- 100*svm_metrics(data)$accuracy
n_acc <- 100*svm_metrics(noisy)$accuracy
```

Clean Dataset | Noisy Dataset
-------------|----------------
`r sprintf("%0.1f", d_acc)`% | `r sprintf("%0.1f", n_acc)`%

. . .

**Can we trust this accuracy estimate having only observed one set of measurements?**

## Toy Example: Simulating Measurement Error

Using that same original dataset, we repeat this process 500 times to obtain the following distribution of classifier accuracy:

```{r cache=T, echo=F}
# simulate measurement error on one clean observation
sim_results_me <- run_simulations(data, 500, 1)

# simulate measurement error on one noisy observation
sim_results_noisy <- run_simulations(noisy, 500, 1)

ggplot(sim_results_me) + geom_histogram(aes(x = accuracy), binwidth = 0.01, color = "#000000", fill= "#ffaa33") +
  xlab("Accuracy") +
  ylab("Frequency")

detach(toy)
```

---

\LARGE Can we estimate that distribution if we only observe the noisy dataset?

# Ideas

## What do we want?

- Want a method for estimating uncertainty in classification performance that can be used for any classifier.
In our project:
- We use Gaussian perturbation based on known measurement error
- As a bonus:
  - quantify class stability
  - probabilities for each observation (think, soft classifier via ensemble learning)

## How do we do it?

![pipeline](Gaussian Perturbation Pipeline.png)

# Back to the Toy Example

## Toy Example: Estimating Variance Without the Clean Data

```{r echo=F, fig.height=3}
ggplot(bind_rows(
  sim_results_me %>% add_column(observed = "clean"),
  sim_results_noisy %>% add_column(observed = "noisy")
)) + geom_boxplot(aes(x = observed, y = accuracy))
```

- The standard deviation of the "clean" distribution (what we want to estimate) is `r sprintf("%0.5f", sd(sim_results_me$accuracy))`.
- The standard deviation of the "noisy" distribution is `r sprintf("%0.5f", sd(sim_results_noisy$accuracy))`.

## Toy Example: A Decision Boundary for Each Perturbed Dataset

```{r echo=F}
plot_data_with_decision_boundaries(noisy, sim_results_noisy)
```

## Toy Example: Purple Points

Averaging predictions over those 500 decision boundaries yields a softened classifier:

```{r echo=F}
classified <- soft_classify_set(sim_results_noisy, noisy)

ggplot(classified) +
  geom_point(aes(x = x1, y = x2, color = p), size = 2) +
  scale_color_gradient(low = "#cc0011", high = "#223399") +
  xlim(-10, 10) +
  ylim(-10, 10)

# KILL THE TOY
detach(toy)
```

# Real Data

## Astronomy

Here's some data from astronomers [@example2014].

. . .

There are literally rocket scientists.

. . .

Just kidding. They don't have rockets. **At Goddard, ppl had bumper stickers that said "It literally is rocket science.**

. . .

But I bet they wish they did.

## Results

In our testing, our method performs amazingly. It is so good.
Show confidence intervals
Show probabiities? i.e. if all perturbed sets predict obs 1 to be class 1, prob = 1

## Future directions + related work
- Gaussian perturbation in clustering setting (mostly used for cluster stability)
  - Note (remove text) In literature, usually choose a fixed variance. Here, use known measurement error so that our measure of cluster stability is essentially a function of the known measurement error
- tbd
  
# The End

## Bibliography

\fontsize{7}{7}\selectfont
